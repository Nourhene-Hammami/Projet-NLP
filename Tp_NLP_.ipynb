{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Première partie : Collecte de données avec scraping**"
      ],
      "metadata": {
        "id": "rGm_FnciC_am"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5CvtZR8C1GN"
      },
      "outputs": [],
      "source": [
        "import requests  # Pour faire des requêtes HTTP\n",
        "from bs4 import BeautifulSoup  # Pour parser le contenu HTML\n",
        "import csv  # Pour écrire dans un fichier CSV\n",
        "import codecs  # Pour gérer l'encodage des fichiers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "# Créer un fichier CSV et définir l'en-tête\n",
        "with codecs.open('job_offers.csv', 'w', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Job Title\", \"Company\", \"Job Type\", \"Job Description\", \"Publication Date\", \"Salary\"])\n"
      ],
      "metadata": {
        "id": "X_zvzLJeXLkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "url = 'https://www.simplyhired.com/search?q=data+science'\n",
        "response = requests.get(url)"
      ],
      "metadata": {
        "id": "xzX1dS2Tntek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Check if pagination element exists\n",
        "    pagination_element = soup.select_one('.pagination')\n",
        "    if pagination_element:\n",
        "        num_pages = int(pagination_element.text.strip().split()[-1])\n",
        "    else:\n",
        "        num_pages = 1  # Assume only one page if pagination not found\n",
        "        print(\"Pagination not found. Assuming only one page.\")\n",
        "\n",
        "    # Boucle pour parcourir toutes les pages\n",
        "    for page in range(1, num_pages + 1):\n",
        "        page_url = f'https://www.simplyhired.com/search?q=data+science&pn={page}'\n",
        "        response = requests.get(page_url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Suite du code pour extraire les données\n"
      ],
      "metadata": {
        "id": "SlNVc4--foCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "soup = BeautifulSoup(response.content, 'html.parser')"
      ],
      "metadata": {
        "id": "xqkBh7L1fybk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7\n",
        "job_cards = soup.findAll('div', class_='SerpJob-jobCard')\n"
      ],
      "metadata": {
        "id": "lIqIjHAagdka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8\n",
        "for job in job_cards:\n",
        "    job_title = job.find('h2', class_='jobposting-title').text.strip() if job.find('h2', class_='jobposting-title') else 'N/A'\n",
        "    company = job.find('span', class_='jobposting-company').text.strip() if job.find('span', class_='jobposting-company') else 'N/A'\n",
        "    job_type = job.find('span', class_='jobposting-jobtype').text.strip() if job.find('span', class_='jobposting-jobtype') else 'N/A'\n",
        "    job_description = job.find('p', class_='jobposting-snippet').text.strip() if job.find('p', class_='jobposting-snippet') else 'N/A'\n",
        "    publication_date = job.find('span', class_='jobposting-postdate').text.strip() if job.find('span', class_='jobposting-postdate') else 'N/A'\n",
        "    salary = job.find('span', class_='jobposting-salary').text.strip() if job.find('span', class_='jobposting-salary') else 'N/A'\n",
        "\n",
        "\n",
        "\n",
        "#9\n",
        "    # Sauvegarde dans le CSV\n",
        "    with codecs.open('job_offers.csv', 'a', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([job_title, company, job_type, job_description, publication_date, salary])\n"
      ],
      "metadata": {
        "id": "4_jfwOCRgfPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deuxième partie : Créer un chatbot avec python**"
      ],
      "metadata": {
        "id": "P1aGcoicsFQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importer les bibliothèques nécessaires\n",
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import string  # pour traiter les chaînes de texte standard en Python\n",
        "\n",
        "# Question 1 : Télécharger le dataset chabot.txt\n",
        "with open('/content/Chatbot.txt', 'r', encoding='utf-8') as file:\n",
        "    raw_text = file.read()\n",
        "\n",
        "#Question 2 : Transformer le texte en minuscules\n",
        "raw_text = raw_text.lower()\n",
        "# Afficher un extrait du texte transformé\n",
        "print(raw_text[:1000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw6Se7BM-ToQ",
        "outputId": "94c42554-d71d-4027-834e-6cedad983346"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: since the industrial revolution, the global annual temperature has increased in total by a little more than 1 degree celsius, or about 2 degrees fahrenheit. between 1880—the year that accurate recordkeeping began—and 1980, it rose on average by 0.07 degrees celsius (0.13 degrees fahrenheit) every 10 years. since 1981, however, the rate of increase has more than doubled: for the last 40 years, we’ve seen the global annual temperature rise by 0.18 degrees celsius, or 0.32 degrees fahrenheit, per decade.\n",
            "\n",
            "the result? a planet that has never been hotter. nine of the 10 warmest years since 1880 have occurred since 2005—and the 5 warmest years on record have all occurred since 2015. climate change deniers have argued that there has been a “pause” or a “slowdown” in rising global temperatures, but numerous studies, including a 2018 paper published in the journal environmental research letters, have disproved this claim. the impacts of global warming are already harming people around the wo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "# Télécharger le module punkt pour la tokenization\n",
        "nltk.download('punkt')\n",
        "# Télécharger le module wordnet pour le vocabulaire anglais\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRvp2ulJTPQ1",
        "outputId": "793cfa85-b153-46ab-db63-c7224b4b35e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4 :Tokenisation\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Générer les tokens sous forme de phrases\n",
        "sentence_tokens = sent_tokenize(raw_text)\n",
        "\n",
        "# Générer les tokens sous forme de mots\n",
        "word_tokens = word_tokenize(raw_text)\n",
        "\n",
        "# Afficher quelques exemples de tokens\n",
        "print(\"Exemples de tokens sous forme de phrases :\")\n",
        "print(sentence_tokens[:5])  # Affiche les 5 premières phrases\n",
        "\n",
        "print(\"\\nExemples de tokens sous forme de mots :\")\n",
        "print(word_tokens[:20])  # Affiche les 20 premiers mots\n"
      ],
      "metadata": {
        "id": "dkjzTPIFsPGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11790083-ec14-4731-fee7-e0077b43872e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exemples de tokens sous forme de phrases :\n",
            "['a: since the industrial revolution, the global annual temperature has increased in total by a little more than 1 degree celsius, or about 2 degrees fahrenheit.', 'between 1880—the year that accurate recordkeeping began—and 1980, it rose on average by 0.07 degrees celsius (0.13 degrees fahrenheit) every 10 years.', 'since 1981, however, the rate of increase has more than doubled: for the last 40 years, we’ve seen the global annual temperature rise by 0.18 degrees celsius, or 0.32 degrees fahrenheit, per decade.', 'the result?', 'a planet that has never been hotter.']\n",
            "\n",
            "Exemples de tokens sous forme de mots :\n",
            "['a', ':', 'since', 'the', 'industrial', 'revolution', ',', 'the', 'global', 'annual', 'temperature', 'has', 'increased', 'in', 'total', 'by', 'a', 'little', 'more', 'than']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5 :Lemmatisation & Suppression de la ponctuation\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
        "LemNormalize(raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvYfMR1mVIb4",
        "outputId": "9bc99127-8b56-4059-fd5d-29119f18cb91"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'since',\n",
              " 'the',\n",
              " 'industrial',\n",
              " 'revolution',\n",
              " 'the',\n",
              " 'global',\n",
              " 'annual',\n",
              " 'temperature',\n",
              " 'ha',\n",
              " 'increased',\n",
              " 'in',\n",
              " 'total',\n",
              " 'by',\n",
              " 'a',\n",
              " 'little',\n",
              " 'more',\n",
              " 'than',\n",
              " '1',\n",
              " 'degree',\n",
              " 'celsius',\n",
              " 'or',\n",
              " 'about',\n",
              " '2',\n",
              " 'degree',\n",
              " 'fahrenheit',\n",
              " 'between',\n",
              " '1880—the',\n",
              " 'year',\n",
              " 'that',\n",
              " 'accurate',\n",
              " 'recordkeeping',\n",
              " 'began—and',\n",
              " '1980',\n",
              " 'it',\n",
              " 'rose',\n",
              " 'on',\n",
              " 'average',\n",
              " 'by',\n",
              " '007',\n",
              " 'degree',\n",
              " 'celsius',\n",
              " '013',\n",
              " 'degree',\n",
              " 'fahrenheit',\n",
              " 'every',\n",
              " '10',\n",
              " 'year',\n",
              " 'since',\n",
              " '1981',\n",
              " 'however',\n",
              " 'the',\n",
              " 'rate',\n",
              " 'of',\n",
              " 'increase',\n",
              " 'ha',\n",
              " 'more',\n",
              " 'than',\n",
              " 'doubled',\n",
              " 'for',\n",
              " 'the',\n",
              " 'last',\n",
              " '40',\n",
              " 'year',\n",
              " 'we',\n",
              " '’',\n",
              " 've',\n",
              " 'seen',\n",
              " 'the',\n",
              " 'global',\n",
              " 'annual',\n",
              " 'temperature',\n",
              " 'rise',\n",
              " 'by',\n",
              " '018',\n",
              " 'degree',\n",
              " 'celsius',\n",
              " 'or',\n",
              " '032',\n",
              " 'degree',\n",
              " 'fahrenheit',\n",
              " 'per',\n",
              " 'decade',\n",
              " 'the',\n",
              " 'result',\n",
              " 'a',\n",
              " 'planet',\n",
              " 'that',\n",
              " 'ha',\n",
              " 'never',\n",
              " 'been',\n",
              " 'hotter',\n",
              " 'nine',\n",
              " 'of',\n",
              " 'the',\n",
              " '10',\n",
              " 'warmest',\n",
              " 'year',\n",
              " 'since',\n",
              " '1880',\n",
              " 'have',\n",
              " 'occurred',\n",
              " 'since',\n",
              " '2005—and',\n",
              " 'the',\n",
              " '5',\n",
              " 'warmest',\n",
              " 'year',\n",
              " 'on',\n",
              " 'record',\n",
              " 'have',\n",
              " 'all',\n",
              " 'occurred',\n",
              " 'since',\n",
              " '2015',\n",
              " 'climate',\n",
              " 'change',\n",
              " 'denier',\n",
              " 'have',\n",
              " 'argued',\n",
              " 'that',\n",
              " 'there',\n",
              " 'ha',\n",
              " 'been',\n",
              " 'a',\n",
              " '“',\n",
              " 'pause',\n",
              " '”',\n",
              " 'or',\n",
              " 'a',\n",
              " '“',\n",
              " 'slowdown',\n",
              " '”',\n",
              " 'in',\n",
              " 'rising',\n",
              " 'global',\n",
              " 'temperature',\n",
              " 'but',\n",
              " 'numerous',\n",
              " 'study',\n",
              " 'including',\n",
              " 'a',\n",
              " '2018',\n",
              " 'paper',\n",
              " 'published',\n",
              " 'in',\n",
              " 'the',\n",
              " 'journal',\n",
              " 'environmental',\n",
              " 'research',\n",
              " 'letter',\n",
              " 'have',\n",
              " 'disproved',\n",
              " 'this',\n",
              " 'claim',\n",
              " 'the',\n",
              " 'impact',\n",
              " 'of',\n",
              " 'global',\n",
              " 'warming',\n",
              " 'are',\n",
              " 'already',\n",
              " 'harming',\n",
              " 'people',\n",
              " 'around',\n",
              " 'the',\n",
              " 'world',\n",
              " 'now',\n",
              " 'climate',\n",
              " 'scientist',\n",
              " 'have',\n",
              " 'concluded',\n",
              " 'that',\n",
              " 'we',\n",
              " 'must',\n",
              " 'limit',\n",
              " 'global',\n",
              " 'warming',\n",
              " 'to',\n",
              " '15',\n",
              " 'degree',\n",
              " 'celsius',\n",
              " 'by',\n",
              " '2040',\n",
              " 'if',\n",
              " 'we',\n",
              " 'are',\n",
              " 'to',\n",
              " 'avoid',\n",
              " 'a',\n",
              " 'future',\n",
              " 'in',\n",
              " 'which',\n",
              " 'everyday',\n",
              " 'life',\n",
              " 'around',\n",
              " 'the',\n",
              " 'world',\n",
              " 'is',\n",
              " 'marked',\n",
              " 'by',\n",
              " 'it',\n",
              " 'worst',\n",
              " 'most',\n",
              " 'devastating',\n",
              " 'effect',\n",
              " 'the',\n",
              " 'extreme',\n",
              " 'drought',\n",
              " 'wildfire',\n",
              " 'flood',\n",
              " 'tropical',\n",
              " 'storm',\n",
              " 'and',\n",
              " 'other',\n",
              " 'disaster',\n",
              " 'that',\n",
              " 'we',\n",
              " 'refer',\n",
              " 'to',\n",
              " 'collectively',\n",
              " 'a',\n",
              " 'climate',\n",
              " 'change',\n",
              " 'these',\n",
              " 'effect',\n",
              " 'are',\n",
              " 'felt',\n",
              " 'by',\n",
              " 'all',\n",
              " 'people',\n",
              " 'in',\n",
              " 'one',\n",
              " 'way',\n",
              " 'or',\n",
              " 'another',\n",
              " 'but',\n",
              " 'are',\n",
              " 'experienced',\n",
              " 'most',\n",
              " 'acutely',\n",
              " 'by',\n",
              " 'the',\n",
              " 'underprivileged',\n",
              " 'the',\n",
              " 'economically',\n",
              " 'marginalized',\n",
              " 'and',\n",
              " 'people',\n",
              " 'of',\n",
              " 'color',\n",
              " 'for',\n",
              " 'whom',\n",
              " 'climate',\n",
              " 'change',\n",
              " 'is',\n",
              " 'often',\n",
              " 'a',\n",
              " 'key',\n",
              " 'driver',\n",
              " 'of',\n",
              " 'poverty',\n",
              " 'displacement',\n",
              " 'hunger',\n",
              " 'and',\n",
              " 'social',\n",
              " 'unrest',\n",
              " 'q',\n",
              " 'what',\n",
              " 'cause',\n",
              " 'global',\n",
              " 'warming',\n",
              " 'a',\n",
              " 'global',\n",
              " 'warming',\n",
              " 'occurs',\n",
              " 'when',\n",
              " 'carbon',\n",
              " 'dioxide',\n",
              " 'co2',\n",
              " 'and',\n",
              " 'other',\n",
              " 'air',\n",
              " 'pollutant',\n",
              " 'collect',\n",
              " 'in',\n",
              " 'the',\n",
              " 'atmosphere',\n",
              " 'and',\n",
              " 'absorb',\n",
              " 'sunlight',\n",
              " 'and',\n",
              " 'solar',\n",
              " 'radiation',\n",
              " 'that',\n",
              " 'have',\n",
              " 'bounced',\n",
              " 'off',\n",
              " 'the',\n",
              " 'earth',\n",
              " '’',\n",
              " 's',\n",
              " 'surface',\n",
              " 'normally',\n",
              " 'this',\n",
              " 'radiation',\n",
              " 'would',\n",
              " 'escape',\n",
              " 'into',\n",
              " 'space',\n",
              " 'but',\n",
              " 'these',\n",
              " 'pollutant',\n",
              " 'which',\n",
              " 'can',\n",
              " 'last',\n",
              " 'for',\n",
              " 'year',\n",
              " 'to',\n",
              " 'century',\n",
              " 'in',\n",
              " 'the',\n",
              " 'atmosphere',\n",
              " 'trap',\n",
              " 'the',\n",
              " 'heat',\n",
              " 'and',\n",
              " 'cause',\n",
              " 'the',\n",
              " 'planet',\n",
              " 'to',\n",
              " 'get',\n",
              " 'hotter',\n",
              " 'these',\n",
              " 'heattrapping',\n",
              " 'pollutants—specifically',\n",
              " 'carbon',\n",
              " 'dioxide',\n",
              " 'methane',\n",
              " 'nitrous',\n",
              " 'oxide',\n",
              " 'water',\n",
              " 'vapor',\n",
              " 'and',\n",
              " 'synthetic',\n",
              " 'fluorinated',\n",
              " 'gases—are',\n",
              " 'known',\n",
              " 'a',\n",
              " 'greenhouse',\n",
              " 'gas',\n",
              " 'and',\n",
              " 'their',\n",
              " 'impact',\n",
              " 'is',\n",
              " 'called',\n",
              " 'the',\n",
              " 'greenhouse',\n",
              " 'effect',\n",
              " 'though',\n",
              " 'natural',\n",
              " 'cycle',\n",
              " 'and',\n",
              " 'fluctuation',\n",
              " 'have',\n",
              " 'caused',\n",
              " 'the',\n",
              " 'earth',\n",
              " '’',\n",
              " 's',\n",
              " 'climate',\n",
              " 'to',\n",
              " 'change',\n",
              " 'several',\n",
              " 'time',\n",
              " 'over',\n",
              " 'the',\n",
              " 'last',\n",
              " '800000',\n",
              " 'year',\n",
              " 'our',\n",
              " 'current',\n",
              " 'era',\n",
              " 'of',\n",
              " 'global',\n",
              " 'warming',\n",
              " 'is',\n",
              " 'directly',\n",
              " 'attributable',\n",
              " 'to',\n",
              " 'human',\n",
              " 'activity—specifically',\n",
              " 'to',\n",
              " 'our',\n",
              " 'burning',\n",
              " 'of',\n",
              " 'fossil',\n",
              " 'fuel',\n",
              " 'such',\n",
              " 'a',\n",
              " 'coal',\n",
              " 'oil',\n",
              " 'gasoline',\n",
              " 'and',\n",
              " 'natural',\n",
              " 'gas',\n",
              " 'which',\n",
              " 'result',\n",
              " 'in',\n",
              " 'the',\n",
              " 'greenhouse',\n",
              " 'effect',\n",
              " 'in',\n",
              " 'the',\n",
              " 'united',\n",
              " 'state',\n",
              " 'the',\n",
              " 'largest',\n",
              " 'source',\n",
              " 'of',\n",
              " 'greenhouse',\n",
              " 'gas',\n",
              " 'is',\n",
              " 'transportation',\n",
              " '29',\n",
              " 'percent',\n",
              " 'followed',\n",
              " 'closely',\n",
              " 'by',\n",
              " 'electricity',\n",
              " 'production',\n",
              " '28',\n",
              " 'percent',\n",
              " 'and',\n",
              " 'industrial',\n",
              " 'activity',\n",
              " '22',\n",
              " 'percent',\n",
              " 'curbing',\n",
              " 'dangerous',\n",
              " 'climate',\n",
              " 'change',\n",
              " 'requires',\n",
              " 'very',\n",
              " 'deep',\n",
              " 'cut',\n",
              " 'in',\n",
              " 'emission',\n",
              " 'a',\n",
              " 'well',\n",
              " 'a',\n",
              " 'the',\n",
              " 'use',\n",
              " 'of',\n",
              " 'alternative',\n",
              " 'to',\n",
              " 'fossil',\n",
              " 'fuel',\n",
              " 'worldwide',\n",
              " 'the',\n",
              " 'good',\n",
              " 'news',\n",
              " 'is',\n",
              " 'that',\n",
              " 'country',\n",
              " 'around',\n",
              " 'the',\n",
              " 'globe',\n",
              " 'have',\n",
              " 'formally',\n",
              " 'committed—as',\n",
              " 'part',\n",
              " 'of',\n",
              " 'the',\n",
              " '2015',\n",
              " 'paris',\n",
              " 'climate',\n",
              " 'agreement—to',\n",
              " 'lower',\n",
              " 'their',\n",
              " 'emission',\n",
              " 'by',\n",
              " 'setting',\n",
              " 'new',\n",
              " 'standard',\n",
              " 'and',\n",
              " 'crafting',\n",
              " 'new',\n",
              " 'policy',\n",
              " 'to',\n",
              " 'meet',\n",
              " 'or',\n",
              " 'even',\n",
              " 'exceed',\n",
              " 'those',\n",
              " 'standard',\n",
              " 'the',\n",
              " 'notsogood',\n",
              " 'news',\n",
              " 'is',\n",
              " 'that',\n",
              " 'we',\n",
              " '’',\n",
              " 're',\n",
              " 'not',\n",
              " 'working',\n",
              " 'fast',\n",
              " 'enough',\n",
              " 'to',\n",
              " 'avoid',\n",
              " 'the',\n",
              " 'worst',\n",
              " 'impact',\n",
              " 'of',\n",
              " 'climate',\n",
              " 'change',\n",
              " 'scientist',\n",
              " 'tell',\n",
              " 'u',\n",
              " 'that',\n",
              " 'we',\n",
              " 'need',\n",
              " 'to',\n",
              " 'reduce',\n",
              " 'global',\n",
              " 'carbon',\n",
              " 'emission',\n",
              " 'by',\n",
              " 'a',\n",
              " 'much',\n",
              " 'a',\n",
              " '40',\n",
              " 'percent',\n",
              " 'by',\n",
              " '2030',\n",
              " 'for',\n",
              " 'that',\n",
              " 'to',\n",
              " 'happen',\n",
              " 'the',\n",
              " 'global',\n",
              " 'community',\n",
              " 'must',\n",
              " 'take',\n",
              " 'immediate',\n",
              " 'concrete',\n",
              " 'step',\n",
              " 'to',\n",
              " 'decarbonize',\n",
              " 'electricity',\n",
              " 'generation',\n",
              " 'by',\n",
              " 'equitably',\n",
              " 'transitioning',\n",
              " 'from',\n",
              " 'fossil',\n",
              " 'fuel–based',\n",
              " 'production',\n",
              " 'to',\n",
              " 'renewable',\n",
              " 'energy',\n",
              " 'source',\n",
              " 'like',\n",
              " 'wind',\n",
              " 'and',\n",
              " 'solar',\n",
              " 'to',\n",
              " 'electrify',\n",
              " 'our',\n",
              " 'car',\n",
              " 'and',\n",
              " 'truck',\n",
              " 'and',\n",
              " 'to',\n",
              " 'maximize',\n",
              " 'energy',\n",
              " 'efficiency',\n",
              " 'in',\n",
              " 'our',\n",
              " 'building',\n",
              " 'appliance',\n",
              " 'and',\n",
              " 'industry',\n",
              " 'q',\n",
              " 'how',\n",
              " 'is',\n",
              " 'global',\n",
              " 'warming',\n",
              " 'linked',\n",
              " 'to',\n",
              " 'extreme',\n",
              " 'weather',\n",
              " 'a',\n",
              " 'scientist',\n",
              " 'agree',\n",
              " 'that',\n",
              " 'the',\n",
              " 'earth',\n",
              " '’',\n",
              " 's',\n",
              " 'rising',\n",
              " 'temperature',\n",
              " 'are',\n",
              " 'fueling',\n",
              " 'longer',\n",
              " 'and',\n",
              " 'hotter',\n",
              " 'heat',\n",
              " 'wave',\n",
              " 'more',\n",
              " 'frequent',\n",
              " 'drought',\n",
              " 'heavier',\n",
              " 'rainfall',\n",
              " 'and',\n",
              " 'more',\n",
              " 'powerful',\n",
              " 'hurricane',\n",
              " 'in',\n",
              " '2015',\n",
              " 'for',\n",
              " 'example',\n",
              " 'scientist',\n",
              " 'concluded',\n",
              " 'that',\n",
              " 'a',\n",
              " 'lengthy',\n",
              " 'drought',\n",
              " 'in',\n",
              " 'california—the',\n",
              " 'state',\n",
              " '’',\n",
              " 's',\n",
              " 'worst',\n",
              " 'water',\n",
              " 'shortage',\n",
              " 'in',\n",
              " '1200',\n",
              " 'years—had',\n",
              " 'been',\n",
              " 'intensified',\n",
              " 'by',\n",
              " '15',\n",
              " 'to',\n",
              " '20',\n",
              " 'percent',\n",
              " 'by',\n",
              " 'global',\n",
              " 'warming',\n",
              " 'they',\n",
              " 'also',\n",
              " 'said',\n",
              " 'the',\n",
              " 'odds',\n",
              " 'of',\n",
              " 'similar',\n",
              " 'drought',\n",
              " 'happening',\n",
              " 'in',\n",
              " 'the',\n",
              " 'future',\n",
              " 'had',\n",
              " 'roughly',\n",
              " 'doubled',\n",
              " 'over',\n",
              " 'the',\n",
              " 'past',\n",
              " 'century',\n",
              " 'and',\n",
              " 'in',\n",
              " '2016',\n",
              " 'the',\n",
              " 'national',\n",
              " 'academy',\n",
              " 'of',\n",
              " 'science',\n",
              " 'engineering',\n",
              " 'and',\n",
              " 'medicine',\n",
              " 'announced',\n",
              " 'that',\n",
              " 'we',\n",
              " 'can',\n",
              " 'now',\n",
              " 'confidently',\n",
              " 'attribute',\n",
              " 'some',\n",
              " 'extreme',\n",
              " 'weather',\n",
              " 'event',\n",
              " 'like',\n",
              " 'heat',\n",
              " 'wave',\n",
              " 'drought',\n",
              " 'and',\n",
              " 'heavy',\n",
              " 'precipitation',\n",
              " 'directly',\n",
              " 'to',\n",
              " 'climate',\n",
              " 'change',\n",
              " 'the',\n",
              " 'earth',\n",
              " '’',\n",
              " 's',\n",
              " 'ocean',\n",
              " 'temperature',\n",
              " 'are',\n",
              " 'getting',\n",
              " 'warmer',\n",
              " 'too—which',\n",
              " 'mean',\n",
              " 'that',\n",
              " 'tropical',\n",
              " 'storm',\n",
              " 'can',\n",
              " 'pick',\n",
              " 'up',\n",
              " 'more',\n",
              " 'energy',\n",
              " 'in',\n",
              " 'other',\n",
              " 'word',\n",
              " 'global',\n",
              " 'warming',\n",
              " 'ha',\n",
              " 'the',\n",
              " 'ability',\n",
              " 'to',\n",
              " 'turn',\n",
              " 'a',\n",
              " 'category',\n",
              " '3',\n",
              " 'storm',\n",
              " 'into',\n",
              " 'a',\n",
              " 'more',\n",
              " 'dangerous',\n",
              " 'category',\n",
              " '4',\n",
              " 'storm',\n",
              " 'in',\n",
              " 'fact',\n",
              " 'scientist',\n",
              " 'have',\n",
              " 'found',\n",
              " 'that',\n",
              " 'the',\n",
              " 'frequency',\n",
              " 'of',\n",
              " 'north',\n",
              " 'atlantic',\n",
              " 'hurricane',\n",
              " 'ha',\n",
              " 'increased',\n",
              " 'since',\n",
              " 'the',\n",
              " 'early',\n",
              " '1980s',\n",
              " 'a',\n",
              " 'ha',\n",
              " 'the',\n",
              " 'number',\n",
              " 'of',\n",
              " 'storm',\n",
              " 'that',\n",
              " 'reach',\n",
              " 'category',\n",
              " '4',\n",
              " 'and',\n",
              " '5',\n",
              " 'the',\n",
              " '2020',\n",
              " 'atlantic',\n",
              " 'hurricane',\n",
              " 'season',\n",
              " 'included',\n",
              " 'a',\n",
              " 'recordbreaking',\n",
              " '30',\n",
              " 'tropical',\n",
              " 'storm',\n",
              " '6',\n",
              " 'major',\n",
              " 'hurricane',\n",
              " 'and',\n",
              " '13',\n",
              " 'hurricane',\n",
              " 'altogether',\n",
              " 'with',\n",
              " 'increased',\n",
              " 'intensity',\n",
              " 'come',\n",
              " 'increased',\n",
              " 'damage',\n",
              " 'and',\n",
              " 'death',\n",
              " 'the',\n",
              " 'united',\n",
              " 'state',\n",
              " 'saw',\n",
              " 'an',\n",
              " 'unprecedented',\n",
              " '22',\n",
              " 'weather',\n",
              " 'and',\n",
              " 'climate',\n",
              " 'disaster',\n",
              " 'that',\n",
              " 'caused',\n",
              " 'at',\n",
              " 'least',\n",
              " 'a',\n",
              " 'billion',\n",
              " 'dollar',\n",
              " '’',\n",
              " 'worth',\n",
              " 'of',\n",
              " 'damage',\n",
              " 'in',\n",
              " '2020',\n",
              " 'but',\n",
              " '2017',\n",
              " 'wa',\n",
              " 'the',\n",
              " 'costliest',\n",
              " 'on',\n",
              " 'record',\n",
              " 'and',\n",
              " 'among',\n",
              " 'the',\n",
              " 'deadliest',\n",
              " 'a',\n",
              " 'well',\n",
              " 'taken',\n",
              " 'together',\n",
              " 'that',\n",
              " 'year',\n",
              " 'tropical',\n",
              " 'storm',\n",
              " 'including',\n",
              " 'hurricane',\n",
              " 'harvey',\n",
              " 'irma',\n",
              " 'and',\n",
              " 'maria',\n",
              " 'caused',\n",
              " 'nearly',\n",
              " '300',\n",
              " 'billion',\n",
              " 'in',\n",
              " 'damage',\n",
              " 'and',\n",
              " 'led',\n",
              " 'to',\n",
              " 'more',\n",
              " 'than',\n",
              " '3300',\n",
              " 'fatality',\n",
              " 'the',\n",
              " 'impact',\n",
              " 'of',\n",
              " 'global',\n",
              " 'warming',\n",
              " 'are',\n",
              " 'being',\n",
              " 'felt',\n",
              " 'everywhere',\n",
              " 'extreme',\n",
              " 'heat',\n",
              " 'wave',\n",
              " 'have',\n",
              " 'caused',\n",
              " 'ten',\n",
              " 'of',\n",
              " 'thousand',\n",
              " 'of',\n",
              " 'death',\n",
              " 'around',\n",
              " 'the',\n",
              " 'world',\n",
              " 'in',\n",
              " 'recent',\n",
              " 'year',\n",
              " 'and',\n",
              " 'in',\n",
              " 'an',\n",
              " 'alarming',\n",
              " 'sign',\n",
              " 'of',\n",
              " 'event',\n",
              " 'to',\n",
              " 'come',\n",
              " 'antarctica',\n",
              " 'ha',\n",
              " 'lost',\n",
              " 'nearly',\n",
              " 'four',\n",
              " 'trillion',\n",
              " 'metric',\n",
              " 'ton',\n",
              " 'of',\n",
              " 'ice',\n",
              " 'since',\n",
              " 'the',\n",
              " '1990s',\n",
              " 'the',\n",
              " 'rate',\n",
              " 'of',\n",
              " 'loss',\n",
              " 'could',\n",
              " 'speed',\n",
              " 'up',\n",
              " 'if',\n",
              " 'we',\n",
              " 'keep',\n",
              " 'burning',\n",
              " 'fossil',\n",
              " 'fuel',\n",
              " 'at',\n",
              " 'our',\n",
              " 'current',\n",
              " 'pace',\n",
              " 'some',\n",
              " 'expert',\n",
              " 'say',\n",
              " 'causing',\n",
              " 'sea',\n",
              " 'level',\n",
              " 'to',\n",
              " 'rise',\n",
              " 'several',\n",
              " 'meter',\n",
              " 'in',\n",
              " 'the',\n",
              " 'next',\n",
              " '50',\n",
              " 'to',\n",
              " '150',\n",
              " 'year',\n",
              " 'and',\n",
              " 'wreaking',\n",
              " 'havoc',\n",
              " 'on',\n",
              " 'coastal',\n",
              " 'community',\n",
              " 'worldwide',\n",
              " 'subscribe',\n",
              " 'to',\n",
              " 'top',\n",
              " 'of',\n",
              " 'mind',\n",
              " 'we',\n",
              " 'deliver',\n",
              " 'story',\n",
              " 'that',\n",
              " 'help',\n",
              " 'break',\n",
              " 'down',\n",
              " 'climate',\n",
              " 'topic',\n",
              " 'and',\n",
              " 'bring',\n",
              " 'you',\n",
              " 'inspiration',\n",
              " 'to',\n",
              " 'keep',\n",
              " 'up',\n",
              " 'your',\n",
              " 'activism',\n",
              " 'get',\n",
              " 'the',\n",
              " 'latest',\n",
              " 'fresh',\n",
              " 'in',\n",
              " 'your',\n",
              " 'inbox',\n",
              " 'each',\n",
              " 'tuesday',\n",
              " 'enter',\n",
              " 'email',\n",
              " 'subscribe',\n",
              " 'we',\n",
              " 'are',\n",
              " 'committed',\n",
              " 'to',\n",
              " 'protecting',\n",
              " 'your',\n",
              " 'privacy',\n",
              " 'and',\n",
              " 'will',\n",
              " 'never',\n",
              " 'sell',\n",
              " 'exchange',\n",
              " 'or',\n",
              " 'rent',\n",
              " 'your',\n",
              " 'email',\n",
              " 'address',\n",
              " 'q',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def greeting(sentence):\n",
        "  for word in sentence.split():\n",
        "   if word.lower() in GREETING_INPUTS:\n",
        "     return random.choice(GREETING_RESPONSES)\n",
        "\n",
        "greeting('hi')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "W6m4pj0kepZP",
        "outputId": "882d07a0-c326-4016-d03a-5caf2c16bf96"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hey'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "9FqhNvbgf2-Y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour trouver la réponse la plus similaire\n",
        "def response(user_response):\n",
        "    robo_response = ''\n",
        "    # Ajouter la réponse utilisateur aux tokens de phrase\n",
        "    sentence_tokens.append(user_response)\n",
        "\n",
        "    # Initialiser TfidfVectorizer\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "\n",
        "    # Appliquer TfidfVectorizer\n",
        "    tfidf = TfidfVec.fit_transform(sentence_tokens)\n",
        "\n",
        "    # Calculer la similarité cosinus\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "\n",
        "    # Trouver l'index de la réponse la plus similaire\n",
        "    idx = vals.argsort()[0][-2]\n",
        "\n",
        "    # Aplatir la matrice des valeurs\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "\n",
        "    # Récupérer la deuxième valeur la plus élevée de la similarité cosinus\n",
        "    req_tfidf = flat[-2]\n",
        "\n",
        "    if req_tfidf == 0:\n",
        "        robo_response = \"I am sorry! I don't understand you.\"\n",
        "    else:\n",
        "        robo_response = sentence_tokens[idx]\n",
        "\n",
        "    # Retirer la réponse utilisateur ajoutée aux tokens de phrase\n",
        "    sentence_tokens.pop(-1)\n",
        "\n",
        "    return robo_response\n",
        "\n",
        "# Exemple d'utilisation\n",
        "response('hi')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "Hltr7PE4gvzV",
        "outputId": "f861f5b3-3d03-4f2a-fa39-79eaf5854ec1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I am sorry! I don't understand you.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main loop\n",
        "flag = True\n",
        "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
        "while(flag):\n",
        "    user_response = input()\n",
        "    user_response = user_response.lower()\n",
        "    if(user_response != 'bye'):\n",
        "        if(user_response == 'thanks' or user_response == 'thank you'):\n",
        "            flag = False\n",
        "            print(\"ROBO: You are welcome..\")\n",
        "        else:\n",
        "            if(greeting(user_response) is not None):\n",
        "                print(\"ROBO: \" + greeting(user_response))\n",
        "            else:\n",
        "                print(\"ROBO: \", end=\"\")\n",
        "                print(response(user_response))\n",
        "    else:\n",
        "        flag = False\n",
        "        print(\"ROBO: Bye! take care..\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oswR_n0ZnZrc",
        "outputId": "a37288e1-8b42-4147-ba11-d90fa0deb95d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
            "hello\n",
            "ROBO: hi\n",
            "ca va\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROBO: I am sorry! I don't understand you.\n",
            "why\n",
            "ROBO: I am sorry! I don't understand you.\n",
            "thanks\n",
            "ROBO: You are welcome..\n"
          ]
        }
      ]
    }
  ]
}